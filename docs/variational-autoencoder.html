<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Variational Autoencoder | My first Book</title>
  <meta name="description" content="Chapter 2 Variational Autoencoder | My first Book" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Variational Autoencoder | My first Book" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://guang-yu-zhu.github.io/FirstBook/" />
  
  
  <meta name="github-repo" content="guang-yu-zhu/FirstBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Variational Autoencoder | My first Book" />
  
  
  

<meta name="author" content="Guangyu Zhu" />


<meta name="date" content="2021-03-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">My first book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="variational-autoencoder.html"><a href="variational-autoencoder.html"><i class="fa fa-check"></i><b>2</b> Variational Autoencoder</a>
<ul>
<li class="chapter" data-level="2.1" data-path="variational-autoencoder.html"><a href="variational-autoencoder.html#intuition"><i class="fa fa-check"></i><b>2.1</b> Intuition</a></li>
<li class="chapter" data-level="2.2" data-path="variational-autoencoder.html"><a href="variational-autoencoder.html#statisical-motivation"><i class="fa fa-check"></i><b>2.2</b> Statisical motivation}</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="variational-autoencoder.html"><a href="variational-autoencoder.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="variational-autoencoder.html"><a href="variational-autoencoder.html#visualization-of-latent-space"><i class="fa fa-check"></i><b>2.3</b> Visualization of latent space</a></li>
<li class="chapter" data-level="2.4" data-path="variational-autoencoder.html"><a href="variational-autoencoder.html#variational-autoencoders-as-a-generative-model"><i class="fa fa-check"></i><b>2.4</b> Variational autoencoders as a generative model</a></li>
<li class="chapter" data-level="" data-path="variational-autoencoder.html"><a href="variational-autoencoder.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">My first Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variational-autoencoder" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Variational Autoencoder</h1>
<p>A variational autoencoder (VAE) <span class="citation">(<a href="variational-autoencoder.html#ref-kingma2013auto" role="doc-biblioref">Kingma and Welling 2013</a>)</span> provides a probabilistic manner for describing an observation in latent space. Thus, rather than building an encoder which outputs a single value to describe each latent state attribute, we’ll formulate our encoder to describe a probability distribution for each latent attribute.</p>
<div id="intuition" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Intuition</h2>
<p>To provide an example, let’s suppose we’ve trained an autoencoder model on a large dataset of faces with a encoding dimension of 6. An ideal autoencoder will learn descriptive attributes of faces such as skin color, whether or not the person is wearing glasses, etc. in an attempt to describe an observation in some compressed representation.</p>
<p><img src="figures/vae1.png" /></p>
<p>In the example above, we’ve described the input image in terms of its latent attributes using a single value to describe each attribute. However, we may prefer to represent each latent attribute as a range of possible values. For instance, what single value would you assign for the smile attribute if you feed in a photo of the Mona Lisa? Using a variational autoencoder, we can describe latent attributes in probabilistic terms.</p>
<p><img src="figures/vae2.png" /></p>
<p>With this approach, we’ll now represent each latent attribute for a given input as a probability distribution. When decoding from the latent state, we’ll randomly sample from each latent state distribution to generate a vector as input for our decoder model.</p>
<p><img src="figures/vae3.png" /></p>
<p>By constructing our encoder model to output a range of possible values (a statistical distribution) from which we’ll randomly sample to feed into our decoder model, we’re essentially enforcing a continuous, smooth latent space representation. For any sampling of the latent distributions, we’re expecting our decoder model to be able to accurately reconstruct the input. Thus, values which are nearby to one another in latent space should correspond with very similar reconstructions.</p>
<p><img src="figures/vae4.png" /></p>
</div>
<div id="statisical-motivation" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Statisical motivation}</h2>
<p>Suppose that there exists some hidden variable <span class="math inline">\(z\)</span> which generates an observation <span class="math inline">\(x\)</span>.</p>
<p><img src="figures/vae5.png" style="width:10.0%" /></p>
<p>We can only see <span class="math inline">\(x\)</span>, but we would like to infer the characteristics of <span class="math inline">\(z\)</span>. In other words, we’d like to compute <span class="math inline">\(p\left( {z|x} \right)\)</span>.
<span class="math display">\[
p\left( z|x \right) = \frac{p\left( {x|z} \right)p\left( z \right)}{p\left( x \right)}
\]</span>
Unfortunately, computing <span class="math inline">\(p\left( x \right)\)</span> is quite difficult. This usually turns out to be an intractable distribution. However, we can apply variational inference to estimate this value.</p>
<p>Let’s approximate <span class="math inline">\(p\left( {z|x} \right)\)</span> by another distribution <span class="math inline">\(q\left( {z|x} \right)\)</span> which we’ll define such that it has a tractable distribution. If we can define the parameters of <span class="math inline">\(q\left( {z|x} \right)\)</span> such that it is very similar to <span class="math inline">\(p\left( {z|x} \right)\)</span>, we can use it to perform approximate inference of the intractable distribution.</p>
<p>Recall that the KL divergence is a measure of difference between two probability distributions. Thus, if we wanted to ensure that <span class="math inline">\(q\left( {z|x} \right)\)</span> was similar to <span class="math inline">\(p\left( {z|x} \right)\)</span>, we could minimize the KL divergence between the two distributions.
<span class="math display">\[\min KL\left( {q\left( {z|x} \right)||p\left( {z|x} \right)} \right)\]</span>
We can minimize the above expression by maximizing the following:
<span class="math display">\[{E_{q\left( {z|x} \right)}}\log p\left( {x|z} \right) - KL\left( {q\left( {z|x} \right)||p\left( z \right)} \right)
\]</span>
The first term represents the reconstruction likelihood and the second term ensures that our learned distribution <span class="math inline">\(q\)</span> is similar to the true prior distribution <span class="math inline">\(p\)</span>.</p>
<p>We can further construct this model into a neural network architecture where the encoder model learns a mapping from <span class="math inline">\(x\)</span> to <span class="math inline">\(z\)</span> and the decoder model learns a mapping from <span class="math inline">\(z\)</span> back to <span class="math inline">\(x\)</span>.</p>
<p><img src="figures/vae6.png" style="width:90.0%" /></p>
<p>Our loss function for this network will consist of two terms, one which penalizes reconstruction error (which can be thought of maximizing the reconstruction likelihood as discussed earlier) and a second term which encourages our learned distribution <span class="math inline">\({q\left( {z|x} \right)}\)</span> to be similar to the true prior distribution <span class="math inline">\({p\left( z \right)}\)</span>, which we’ll assume follows a unit Gaussian distribution, for each dimension <span class="math inline">\(j\)</span> of the latent space.
<span class="math display">\[
{\cal L}\left( x,\hat x \right) + \sum\limits_j KL\left( q_j\left( z|x \right) || p\left( z \right) \right)
\]</span></p>
<div id="implementation" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Implementation</h3>
<p>Rather than directly outputting values for the latent state as we would in a standard autoencoder, the VAE will output parameters describing a distribution for each dimension in the latent space. Since we’re assuming that our prior follows a normal distribution, we’ll output two vectors describing the mean and variance of the latent state distributions. If we were to build a true multivariate Gaussian model, we’d need to define a covariance matrix describing how each of the dimensions are correlated. However, we’ll make a simplifying assumption that our covariance matrix only has nonzero values on the diagonal, allowing us to describe this information in a simple vector.
Our decoder model will then generate a latent vector by sampling from these defined distributions and proceed to develop a reconstruction of the original input.</p>
<p><img src="figures/vae7.png" style="width:90.0%" /></p>
<p>However, this sampling process requires some extra attention. When training the model, we need to be able to calculate the relationship of each parameter in the network with respect to the final output loss using a technique known as backpropagation. However, we simply cannot do this for a random sampling process. Fortunately, we can leverage a clever idea known as the “reparameterization trick” which suggests that we randomly sample <span class="math inline">\(\varepsilon\)</span> from a unit Gaussian, and then shift the randomly sampled <span class="math inline">\(\varepsilon\)</span> by the latent distribution’s mean <span class="math inline">\(\mu\)</span> and scale it by the latent distribution’s variance <span class="math inline">\(\sigma\)</span>.</p>
<p><img src="figures/vae8.png" style="width:90.0%" /></p>
<p>With this reparameterization, we can now optimize the parameters of the distribution while still maintaining the ability to randomly sample from that distribution.</p>
<p><img src="figures/vae9.png" style="width:40.0%" /></p>
</div>
</div>
<div id="visualization-of-latent-space" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Visualization of latent space</h2>
<p>To understand the implications of a variational autoencoder model and how it differs from standard autoencoder architectures, it’s useful to examine the latent space.
The main benefit of a variational autoencoder is that we’re capable of learning smooth latent state representations of the input data. For standard autoencoders, we simply need to learn an encoding which allows us to reproduce the input. As you can see in the left-most figure, focusing only on reconstruction loss does allow us to separate out the classes (in this case, MNIST digits) which should allow our decoder model the ability to reproduce the original handwritten digit, but there’s an uneven distribution of data within the latent space. In other words, there are areas in latent space which don’t represent any of our observed data.</p>
<p><img src="figures/vae10.png" style="width:90.0%" /></p>
<p>On the flip side, if we only focus only on ensuring that the latent distribution is similar to the prior distribution (through our KL divergence loss term), we end up describing every observation using the same unit Gaussian, which we subsequently sample from to describe the latent dimensions visualized. This effectively treats every observation as having the same characteristics; in other words, we’ve failed to describe the original data.</p>
<p>However, when the two terms are optimized simultaneously, we’re encouraged to describe the latent state for an observation with distributions close to the prior but deviating when necessary to describe salient features of the input.</p>
<p><img src="figures/vae11.png" style="width:90.0%" /></p>
<p>If we observe that the latent distributions appear to be very tight, we may decide to give higher weight to the KL divergence term with a parameter <span class="math inline">\(\beta&gt;1\)</span>, encouraging the network to learn broader distributions. This simple insight has led to the growth of a new class of models - disentangled variational autoencoders. As it turns out, by placing a larger emphasis on the KL divergence term we’re also implicitly enforcing that the learned latent dimensions are uncorrelated (through our simplifying assumption of a diagonal covariance matrix).</p>
<p><span class="math display">\[{\cal L}\left( x,\hat{x} \right) + \beta \sum\limits_j KL\left( q_j\left( z|x \right) ||  N(0,1) \right)\]</span></p>
</div>
<div id="variational-autoencoders-as-a-generative-model" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Variational autoencoders as a generative model</h2>
<p>By sampling from the latent space, we can use the decoder network to form a generative model capable of creating new data similar to what was observed during training. Specifically, we’ll sample from the prior distribution <span class="math inline">\({p\left( z \right)}\)</span> which we assumed follows a unit Gaussian distribution.</p>
<p>The figure below visualizes the data generated by the decoder network of a variational autoencoder trained on the MNIST handwritten digits dataset. Here, we’ve sampled a grid of values from a two-dimensional Gaussian and displayed the output of our decoder network.</p>
<p><img src="figures/vae12.png" style="width:70.0%" /></p>
<p>As you can see, the distinct digits each exist in different regions of the latent space and smoothly transform from one digit to another. This smooth transformation can be quite useful when you’d like to interpolate between two observations, such as this recent example where</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-kingma2013auto" class="csl-entry">
Kingma, Diederik P, and Max Welling. 2013. <span>“Auto-Encoding Variational Bayes.”</span> <em>arXiv Preprint arXiv:1312.6114</em>.
</div>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
